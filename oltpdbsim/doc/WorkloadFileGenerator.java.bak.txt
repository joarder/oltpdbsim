/**
 * @author Joarder Kamal
 */

package main.java.repartition;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.OutputStreamWriter;
import java.io.Writer;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;
import java.util.TreeMap;
import java.util.TreeSet;
import java.util.Map.Entry;
import main.java.cluster.Cluster;
import main.java.cluster.Data;
import main.java.entry.Global;
import main.java.utils.Utility;
import main.java.utils.graph.CompressedHEdge;
import main.java.utils.graph.CompressedVertex;
import main.java.utils.graph.SimpleVertex;
import main.java.workload.Transaction;
import main.java.workload.WorkloadBatch;
import org.apache.commons.lang3.StringUtils;

public class WorkloadFileGenerator {

	// Assigns Shadow HMetis Data Id only for Graph and Hypergraph partitioning
	public void assignShadowDataId(Cluster cluster, WorkloadBatch wb) {
		
		int shadow_id = 1;
		int tr_count = 0;

		for(Entry<Integer, Map<Integer, Transaction>> entry : wb.getTrMap().entrySet()) {
			for(Entry<Integer, Transaction> tr_entry : entry.getValue().entrySet()) {				
				Transaction transaction = tr_entry.getValue();
				++tr_count;
				
				for(Integer data_id : transaction.getTr_dataSet()) {
					Data data = cluster.getData(data_id);
					
					if(!data.isData_hasShadowId()) {
						wb.getWrl_dataId_shadowId_map().put(data.getData_id(), shadow_id);
						
						data.setData_shadowId(shadow_id);
						data.setData_hasShadowId(true);	
						data.setData_inUse(true);
						
						++shadow_id;					
					}
				} // end -- for()-Data
			} // end -- for()-Transaction
		} // end -- for()-Transaction Types
		
		wb.setWrl_totalDataObjects(shadow_id - 1);
		wb.setWrl_totalTransaction(tr_count);
	}
	
	public boolean generateWorkloadFile(Cluster cluster, WorkloadBatch wb) throws IOException {
		boolean empty = false;

		// Workload file
		String wrl_file_name = wb.getWrl_id()+"-"+Global.simulation+"-"+Global.workloadRepresentation+"-"+Global.wrl_file_name; 
		String wrl_abs_file_name = Global.part_dir+Global.getRunDir()+wrl_file_name;				
		
		File workloadFile = new File(wrl_abs_file_name);
		
		wb.setWrl_file_name(wrl_abs_file_name);
		wb.setWrl_file(workloadFile);
		
		// Fixfile
//		if(!partitioner.equals("gr")) {
//			String wrl_fixfile_name = wb.getWrl_id()+"-"+Global.simulation+"-"+Global.workloadRepresentation+"-"+Global.wrl_fixfile_name;
//			String wrl_abs_fixfile_name = Global.part_dir+Global.getRunDir()+wrl_fixfile_name;
//							
//			File fixFile = new File(wrl_abs_fixfile_name);
//			
//			wb.setWrl_fixfile_name(wrl_abs_fixfile_name);
//			wb.setWrl_fixfile(fixFile);
//		}
		
		// Generates specified workload file for repartitioning
		switch(Global.workloadRepresentation) {
		case "hgr":
			empty = this.generateHGraphWorkloadFile(cluster, wb);			
			//if(!empty) this.generateHGraphFixFile(cluster, wb);
			
			break;
			
		case "chg":
			empty = this.generateCHGraphWorkloadFile(cluster, wb);
			//empty = this.generateCHGraphWorkloadFile(cluster, wb);
			//if(!empty) this.generateCHGraphFixFile(cluster, wb);
			
			break;
			
		case "gr":
			this.generateGraphWorkloadFile(cluster, wb);
			
			break;
		}
		
		return empty;
	}
	
	// Generates Workload File for Hypergraph partitioning
	private boolean generateHGraphWorkloadFile(Cluster cluster, WorkloadBatch wb) {
		
		int hyper_edges = wb.getWrl_totalTransactions();		
		int vertices = wb.getWrl_totalDataObjects();
		int hasTransactionWeight = 1;
		int hasDataWeight = 1;						
		
		Data trData = null;
		
		if(hyper_edges <= 1 ) {
			
			Global.LOGGER.info("Only "+hyper_edges+" hyperedges present in the workload batch.");
			Global.LOGGER.info("Repartitioning will be aborted for this run ...");
			
			return true;
		} else {		
			try {
				wb.getWrl_file().getParentFile().mkdirs();
				wb.getWrl_file().createNewFile();
				
				Writer writer = null;
				
				try {
					writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(wb.getWrl_file()), "utf-8"));
					writer.write(hyper_edges+" "+vertices+" "+hasTransactionWeight+""+hasDataWeight+"\n");
					//Global.LOGGER.info(hyper_edges+" "+vertices+" "+hasTransactionWeight+""+hasDataWeight);
					
					for(Entry<Integer, Map<Integer, Transaction>> entry : wb.getTrMap().entrySet()) {
						for(Entry<Integer, Transaction> tr_entry : entry.getValue().entrySet()) {				
							Transaction tr = tr_entry.getValue();
							
							if(tr.getTr_class() != "green") {
								writer.write(tr.getTr_frequency()*tr.getTr_temporal_weight()+" ");
								//Global.LOGGER.info("@@ "+transaction.getTr_frequency()+"* ");
								
								Iterator<Integer> data =  tr.getTr_dataSet().iterator();
								while(data.hasNext()) {
									trData = cluster.getData(data.next());
									
									writer.write(Integer.toString(trData.getData_shadowId()));							
									
									if(data.hasNext())
										writer.write(" "); 
								} // end -- while() loop
								
								writer.write("\n");						
							} // end -- if()-Transaction Class
						} // end -- for()-Transaction
					} // end -- for()-Transaction-Types
	
					// Writing Data Weight
					Set<Integer> uniqueDataSet = new TreeSet<Integer>();
					int newline = 0;
					
					for(Entry<Integer, Map<Integer, Transaction>> entry : wb.getTrMap().entrySet()) {
						for(Entry<Integer, Transaction> tr_entry : entry.getValue().entrySet()) {				
							Transaction transaction = tr_entry.getValue();
							
							if(transaction.getTr_class() != "green") {
								
								Iterator<Integer> data =  transaction.getTr_dataSet().iterator();							
								
								while(data.hasNext()) {
									trData = cluster.getData(data.next());								
									
									if(!uniqueDataSet.contains(trData.getData_shadowId())) {
										++newline;
										
										writer.write(Integer.toString(trData.getData_weight()));	
										//Global.LOGGER.info("@@ "+trData.getData_weight());
										//writer.write(Integer.toString(1));
										
										if(newline != vertices)
											writer.write("\n");										
										
										uniqueDataSet.add(trData.getData_shadowId());
									}
								}							
							}
						}
					}
					
				} catch(IOException e) {
					e.printStackTrace();
				}finally {
					writer.close();
				}
			} catch (IOException e) {		
				e.printStackTrace();
			}
			
			Global.LOGGER.info("Workload file generation for hypergraph based repartitioning has completed.");
			
			return false;
		}
	}
	
	// Generates Fix Files (Determines whether a Data is movable from its current Partition or not) 
	// for Hypergraph partitioning
	@SuppressWarnings("unused")
	private void generateHGraphFixFile(Cluster cluster, WorkloadBatch wb) {

		Data trData = null;
		
		try {
			wb.getWrl_fixfile().getParentFile().mkdirs();
			wb.getWrl_fixfile().createNewFile();
			
			Writer writer = null;
			
			try {
				writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(wb.getWrl_fixfile()), "utf-8"));
				
				Set<Integer> uniqueDataSet = new TreeSet<Integer>();
				int newline = 0;
				
				for(Entry<Integer, Map<Integer, Transaction>> entry : wb.getTrMap().entrySet()) {
					for(Entry<Integer, Transaction> tr_entry : entry.getValue().entrySet()) {				
						Transaction transaction = tr_entry.getValue();
						
						if(transaction.getTr_class() != "green") {							
							Iterator<Integer> data =  transaction.getTr_dataSet().iterator();							
							
							while(data.hasNext()) {
								trData = cluster.getData(data.next());						
								
								if(!uniqueDataSet.contains(trData.getData_shadowId())) {
									++newline;
									
									if(trData.isData_isMoveable())									
										writer.write(Integer.toString(trData.getData_partition_id() - 1));
									else
										writer.write(Integer.toString(-1));
									
									if(newline != wb.getWrl_totalDataObjects())
										writer.write("\n");										
									
									uniqueDataSet.add(trData.getData_shadowId());
								}
							}
						}
					}					
				}
			} catch(IOException e) {
				e.printStackTrace();
			}finally {
				writer.close();
			}
		} catch (IOException e) {		
			e.printStackTrace();
		}
		
		Global.LOGGER.info("Fixfile generation for hypergraph based repartitioning has completed.");
	}
	
	// Generates Workload File for Graph partitioning
	@SuppressWarnings("unused")
	private boolean generateGraphWorkloadFile1(Cluster cluster, WorkloadBatch wb) throws IOException {
		
		Data trData = null;
		Data trInvolvedData = null;
		Set<Integer> dataIdSet = null;
		Set<Integer> dataSet = new TreeSet<Integer>();
		String content = "";
		
		int edges = 0; // Total number of edges need to be determined
		int vertices = wb.getWrl_totalDataObjects();		
		int hasVertexWeight = 1;
		int hasEdgeWeight = 1;
		int new_line = vertices;
		
		//Global.LOGGER.info("@ tr edges = "+workload.getWrl_totalTransactions());
		
		for(Entry<Integer, Map<Integer, Transaction>> entry : wb.getTrMap().entrySet()) {
			for(Entry<Integer, Transaction> tr_entry : entry.getValue().entrySet()) {				
				Transaction transaction = tr_entry.getValue();
				
				if(transaction.getTr_class() != "green") {
					
					Iterator<Integer> data_id_itr =  transaction.getTr_dataSet().iterator();
					while(data_id_itr.hasNext()) {
						trData = cluster.getData(data_id_itr.next());
											
						if(!dataSet.contains(trData.getData_id())) {
							dataSet.add(trData.getData_id());							
							dataIdSet = new TreeSet<Integer>();
							
							String str = Integer.toString(trData.getData_weight())+" ";							
							//Global.LOGGER.info("@ "+trData.toString()+" | size = "+trData.getData_transactions_involved().size());							
							
							Set<Integer> trSet = wb.getWrl_dataTransactionsInvolved().get(trData.getData_id());							
							if(trSet.size() != 0) {
								for(Integer transaction_id : trSet) {
									Transaction tr = wb.getTransaction(transaction_id);
									
									if(tr != null) {
										for(int trInvolvedDataId : tr.getTr_dataSet()) {
											trInvolvedData = cluster.getData(trInvolvedDataId);													
											
											if(!dataIdSet.contains(trInvolvedDataId) && trInvolvedData.getData_id() != trData.getData_id()) {
												str += Integer.toString(trInvolvedData.getData_shadowId())+" ";							
												str += (tr.getTr_frequency()*tr.getTr_temporal_weight())+" ";
												
												++edges;
												//Global.LOGGER.info("@ edges = "+edges);
												dataIdSet.add(trInvolvedData.getData_id());
											}
										}
									} else {
										Global.LOGGER.error("Null transaction is found !!! "+transaction_id);
									}
								}															
							} else {
								Global.LOGGER.error("No involved transactions !!! "+trData.toString()+" | "+trData.getData_shadowId());
							}
							
							content += StringUtils.stripEnd(str, null);
							
							--new_line;
							
							if(new_line != 0)
								content += "\n";
						}																
					} // end -- while() loop																				
				} // end -- if()-Transaction Class
			} // end -- for()-Transaction
		} // end -- for()-Transaction-Types
		
		//Global.LOGGER.info("@ final edges = "+edges);
		
		/*if(edges/2 <= 1) {
			Global.LOGGER.info("[ALM] Only "+edges+" graph edges have been present in the workload");
			Global.LOGGER.info("[ACT] Simulation will be aborted for this run ...");
			Global.LOGGER.info("@ edges/2 = "+edges/2);
			return true;
		} else {*/		
			try {
				wb.getWrl_file().getParentFile().mkdirs();
				wb.getWrl_file().createNewFile();
				
				Writer writer = null;			
	
				try {
					writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(wb.getWrl_file()), "utf-8"));
					writer.write(vertices+" "+(edges/2)+" "+hasVertexWeight+""+hasEdgeWeight+"\n"+content);

				} catch(IOException e) {
					e.printStackTrace();
				}finally {
					writer.close();
				}
			} catch (IOException e) {		
				e.printStackTrace();
			}
			
			Global.LOGGER.info("Workload file generation for graph based repartitioning has completed.");
			return false;
		//}
	} 
	
	// Generates Workload File for Graph partitioning
	private boolean generateGraphWorkloadFile(Cluster cluster, WorkloadBatch wb) throws IOException {
		
		Map<Integer, Integer> vertex_id_map = new TreeMap<Integer, Integer>();
		int vertex_id = 0;		
		for(SimpleVertex v : wb.gr.getVertices()) {
			vertex_id_map.put(v.getId(), ++vertex_id);
			
			Data data = cluster.getData(v.getId());
			data.setData_shadowId(++vertex_id);
			data.setData_inUse(true);						
		}
		
		int edges = wb.gr.getEdgeCount();
		int vertices = wb.gr.getVertexCount();
		
		// Creating Graph Workload File
		Global.LOGGER.info("Total "+edges+" transactional edges containing "+vertices+" tuples have been identified for repartitioning.");
		Global.LOGGER.info("Generating workload file for graph based repartitioning ...");
				
		int hasEdgeWeight = 1;
		int hasVertexWeight = 1;		
				
		// Write in a file		
		if(edges <= 1) { 
			Global.LOGGER.info("Only "+edges+" edges present in the workload");
			Global.LOGGER.info("Repartitioning will be aborted for this run ...");			
			return true;
			
		}else{		
			try {
				wb.getWrl_file().getParentFile().mkdirs();
				wb.getWrl_file().createNewFile();
				
				Writer writer = null;
				
				try {
					writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(wb.getWrl_file()), "utf-8"));
					writer.write(vertices+" "+edges+" "+hasVertexWeight+""+hasEdgeWeight); // edges/2
				
					for(SimpleVertex v : wb.gr.getVertices()) {
						writer.write(Integer.toString(v.getWeight())+" ");
						
						for(SimpleVertex n : wb.gr.getNeighbors(v)) {
							String n_id = Integer.toString(vertex_id_map.get(n.getId()));
							String n_edge_weight = Integer.toString(wb.gr.findEdge(v, n).getWeight());
							
							writer.write(n_id+" "+n_edge_weight+" ");
						}
						
						writer.write("\n");
					}
					
				} catch(IOException e) {
					e.printStackTrace();
				}finally {
					writer.close();
				}
			} catch (IOException e) {		
				e.printStackTrace();
			}	
			
			Global.LOGGER.info("Workload file generation for graph based repartitioning has completed.");
			return false;
		}		
	}
	
	// Generates Workload File for Compressed Hypergraph based repartitioning
	private boolean generateCHGraphWorkloadFile(Cluster cluster, WorkloadBatch wb) {
		
		Map<CompressedHEdge, Set<CompressedVertex>> vedge = new TreeMap<CompressedHEdge, Set<CompressedVertex>>();
		Set<CompressedVertex> vvertex = new TreeSet<CompressedVertex>();
		
		// Only select the compressed hyperedges having at least two compressed vertices
		for(Entry<CompressedHEdge, Set<CompressedVertex>> entry : wb.hgr.getcHEdges().entrySet()) {
			if(entry.getValue().size() >= 2) {
				vedge.put(entry.getKey(), entry.getValue());
				vvertex.addAll(entry.getValue());				
			}
		}
		
		Map<Integer, Integer> vvertex_id_map = new TreeMap<Integer, Integer>();
		int vvertex_id = 0;
		
		for(CompressedVertex cv : vvertex) {
			vvertex_id_map.put(cv.getId(), ++vvertex_id);
			
			for(Entry<Integer, SimpleVertex> entry : cv.getVSet().entrySet()) {
				Data data = cluster.getData(entry.getValue().getId());
				data.setData_virtual_data_id(cv.getId());
				data.setData_inUse(true);
			}
		}
		
		// Creating Compressed Hyper-graph Workload File
		Global.LOGGER.info("Total "+vedge.size()+" virtual transactions containing "+vvertex.size()+" virtual tuples have been identified for repartitioning.");
		Global.LOGGER.info("Generating workload file for compressed hypergraph based repartitioning ...");
				
		int hasTransactionWeight = 1;
		int hasDataWeight = 1;
				
		// Write in a file		
		if(vedge.size() <= 1) { 
			Global.LOGGER.info("Only "+vedge.size()+" compressed hyperedges present in the compressed workload");
			Global.LOGGER.info("Repartitioning will be aborted for this run ...");			
			return true;
			
		}else{		
			try {
				wb.getWrl_file().getParentFile().mkdirs();
				wb.getWrl_file().createNewFile();
				
				Writer writer = null;
				
				try {
					writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(wb.getWrl_file()), "utf-8"));
					writer.write(vedge.size()+" "+vvertex.size()+" "+hasTransactionWeight+""+hasDataWeight+"\n");
					
					for(Entry<CompressedHEdge, Set<CompressedVertex>> entry : vedge.entrySet()) {
						
						// Writing e' weight
						writer.write(Integer.toString(entry.getKey().getWeight())+" ");
								
						// Writing v' incident on e'
						Iterator<CompressedVertex> cv_iterator =  entry.getValue().iterator();
						while(cv_iterator.hasNext()) {						
							writer.write(Integer.toString(vvertex_id_map.get(cv_iterator.next().getId())));
							
							if(cv_iterator.hasNext())
								writer.write(" "); 
						}
						
						writer.write("\n");		
					}
	
					// Writing v' weight
					int newline = 0;
					
					for(CompressedVertex cv : vvertex) {
						writer.write(Integer.toString(cv.getWeight()));	
							
						if(newline != (vvertex.size()-1))
							writer.write("\n");
							
						++newline;
					}
					
				} catch(IOException e) {
					e.printStackTrace();
				}finally {
					writer.close();
				}
			} catch (IOException e) {		
				e.printStackTrace();
			}	
			
			Global.LOGGER.info("Workload file generation for compressed hypergraph based repartitioning has completed.");
			return false;
		}
	}	
	
	// Generates Workload File for Compressed Hypergraph based repartitioning
	@SuppressWarnings("unused")
	private boolean generateCHGraphWorkloadFile1(Cluster cluster, WorkloadBatch wb) {		
		Global.LOGGER.info("Starting workload compression with CR = "+Global.compressionRatio+" ...");
		
		Map<Integer, Set<Integer>> vedge = new TreeMap<Integer, Set<Integer>>();
		Map<Integer, Integer> vedge_frequency = new TreeMap<Integer, Integer>();
		Map<Integer, Integer> vedge_temporal_weight = new TreeMap<Integer, Integer>();		
		Map<Integer, Integer> vvertex = new TreeMap<Integer, Integer>();
		Map<Integer, Integer> vvertex_hash_pk_map = new TreeMap<Integer, Integer>();
		
		Set<Integer> vvertexSet = null;
		Set<Integer> toBeRemoved = new TreeSet<Integer>();
		
		int vvertex_id = 0;
		
		for(Entry<Integer, Map<Integer, Transaction>> entry : wb.getTrMap().entrySet()) {
			for(Entry<Integer, Transaction> tr_entry : entry.getValue().entrySet()) {				
				Transaction transaction = tr_entry.getValue();
				
				if(transaction.getTr_class() != "green") {					
					int tid = transaction.getTr_id();
					int hash_pk = -1;
					int vv_id = -1;
					
					// Set initial v' frequency
					vedge_frequency.put(tid, transaction.getTr_frequency());
					
					// Set temporal weight
					vedge_temporal_weight.put(tid, transaction.getTr_temporal_weight());
					
					for(Integer data_id : transaction.getTr_dataSet()){						
						// Determine the virtual data id
						hash_pk = Utility.simpleHash(data_id, Global.virtualNodes);
												
						// Storing v'
						if(!vvertex_hash_pk_map.containsKey(hash_pk)) {
							++vvertex_id;
							vvertex_hash_pk_map.put(hash_pk, vvertex_id);
							vv_id = vvertex_id;
							
							vvertex.put(vvertex_id, 1);
							
						} else {
							vv_id = vvertex_hash_pk_map.get(hash_pk);
							
							// Increase the corresponding v' weight
							int v_weight = vvertex.get(vv_id);
							vvertex.put(vv_id, ++v_weight);
						}
						
						// Set the virtual data id
						cluster.getData(data_id).setData_virtual_data_id(vv_id);
						
						// Replace v' by v for each transactions 
						if(vedge.containsKey(tid))
							vedge.get(tid).add(vv_id);
						else{
							vvertexSet = new TreeSet<Integer>();
							vvertexSet.add(vv_id);
							vedge.put(tid, vvertexSet);
						}//end-else				
					}//end-for()
					
					if(vedge.get(tid).size() < 2)
						toBeRemoved.add(tid);
				}//end-if
			}//end-for()
		}//end-for()
		
		Global.LOGGER.info("@ "+vedge.size()+"|"+toBeRemoved.size()+"|vvertex_id="+vvertex_id);
		//Global.LOGGER.info("@ "+vedge);
		//Global.LOGGER.info("@ "+vvertex);
		
		// Pruning duplicate virtual edges and recalculating frequencies		
		for(Entry<Integer, Set<Integer>> vedg : vedge.entrySet()){
			int e1 = vedg.getKey();
			
			for(Entry<Integer, Set<Integer>> ve : vedge.entrySet()){
				int e2 = ve.getKey();
				
				if(e1 != e2){				
					if(vedg.getValue().equals(ve.getValue())){
						//Global.LOGGER.info("*");
						
						if(!toBeRemoved.contains(ve.getKey()))
							toBeRemoved.add(ve.getKey());
						
						int freq = vedge_frequency.get(vedg.getKey());
						vedge_frequency.remove(vedg.getKey());
						vedge_frequency.put(vedg.getKey(), ++freq);
						
						int tw_vedg = wb.getTransaction(vedg.getKey()).getTr_temporal_weight();
						int tw_ve = wb.getTransaction(ve.getKey()).getTr_temporal_weight();
						vedge_temporal_weight.remove(vedg.getKey());
						vedge_temporal_weight.put(vedg.getKey(), (tw_vedg+tw_ve));					
					}
				}
			}
		}
		
		//Global.LOGGER.info(">> "+vedge.size()+"|"+toBeRemoved.size());
				
		// Removing duplicate virtual edges
		for(Integer r : toBeRemoved){
			vedge.remove(r);
			vedge_frequency.remove(r);
			vedge_temporal_weight.remove(r);
		}
		
		// Creating Compressed Hyper-graph Workload File
		Global.LOGGER.info("Total "+vedge.size()+" virtual transactions containing "+vvertex.size()+" virtual tuples have been identified for repartitioning.");
		Global.LOGGER.info("Generating workload file for compressed hypergraph based repartitioning ...");
				
		int hasTransactionWeight = 1;
		int hasDataWeight = 1;						
		
		if(vedge.size() <= 1) { 
			Global.LOGGER.info("Only "+vedge.size()+" compressed hyperedges present in the compressed workload");
			Global.LOGGER.info("Repartitioning will be aborted for this run ...");
			
			return true;
		}else{		
			try {
				wb.getWrl_file().getParentFile().mkdirs();
				wb.getWrl_file().createNewFile();
				
				Writer writer = null;
				
				try {
					writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(wb.getWrl_file()), "utf-8"));
					writer.write(vedge.size()+" "+vvertex.size()+" "+hasTransactionWeight+""+hasDataWeight+"\n");
					
					for(Entry<Integer, Set<Integer>> entry : vedge.entrySet()) {
						
						int freq = vedge_frequency.get(entry.getKey());
						int tempWeight = vedge_temporal_weight.get(entry.getKey());
						
						int vedge_weight = freq*tempWeight;
						//int vedge_weight = 1;
						
						// Writing e' weight
						writer.write(vedge_weight+" ");
								
						Iterator<Integer> v_id_iterator =  entry.getValue().iterator();
						while(v_id_iterator.hasNext()) {						
							writer.write(Integer.toString(v_id_iterator.next()));							
							
							if(v_id_iterator.hasNext())
								writer.write(" "); 
						}
						
						writer.write("\n");		
					}
	
					// Writing v' weight
					int newline = 0;
					
					for(Entry<Integer, Integer> entry : vvertex.entrySet()) {
						//writer.write(Integer.toString(entry.getValue()));
						writer.write(Integer.toString(1));
							
						if(newline != (vvertex.size()-1))
							writer.write("\n");
							
						++newline;
					}
					
				} catch(IOException e) {
					e.printStackTrace();
				}finally {
					writer.close();
				}
			} catch (IOException e) {		
				e.printStackTrace();
			}	
			
			Global.LOGGER.info("Workload file generation for compressed hypergraph based repartitioning has completed.");
			return false;
		}				
	}	
	
	// Generate Fix File for Compressed Hyper-graph partitioning 
	@SuppressWarnings("unused")
	private void generateCHGraphFixFile(Cluster cluster, WorkloadBatch wb) {
		
		Data trData = null;
		
		try {
			wb.getWrl_fixfile().getParentFile().mkdirs();
			wb.getWrl_fixfile().createNewFile();
			
			Writer writer = null;
			
			try {
				writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(wb.getWrl_fixfile()), "utf-8"));
				
				Set<Integer> uniqueDataSet = new TreeSet<Integer>();
				int newline = 0;
				
				for(Entry<Integer, Map<Integer, Transaction>> entry : wb.getTrMap().entrySet()) {
					for(Entry<Integer, Transaction> tr_entry : entry.getValue().entrySet()) {				
						Transaction transaction = tr_entry.getValue();
						
						if(transaction.getTr_class() != "green") {							
							Iterator<Integer> data =  transaction.getTr_dataSet().iterator();							
							
							while(data.hasNext()) {
								trData = cluster.getData(data.next());								
								
								if(!uniqueDataSet.contains(trData.getData_shadowId())) {
									++newline;
									
									if(trData.isData_isMoveable())									
										writer.write(Integer.toString(trData.getData_partition_id()));
									else
										writer.write(Integer.toString(-1));
									
									if(newline != wb.getWrl_totalDataObjects())
										writer.write("\n");										
									
									uniqueDataSet.add(trData.getData_shadowId());
								}
							}
						}
					}					
				}
			} catch(IOException e) {
				e.printStackTrace();
			}finally {
				writer.close();
			}
		} catch (IOException e) {		
			e.printStackTrace();
		}
		
		Global.LOGGER.info("Fixfile generation for compressed hypergraph based repartitioning has completed.");
	}
}