/* *****************************************************************************
 * Simulation of a simple M/M/1 queue (Grocery).
 * Class: Simulation - Modeling and Performance Analysis 
 *        with Discrete-Transaction Simulation
 *        RWTH Aachen, Germany, 2007
 *        
 * Author: Dr. Mesut GÃ¼nes, guenes@cs.rwth-aachen.de        
 * 
 * Notice: 
 * This code is based on the example presented in the very nice book of 
 * Jerry Banks, John S. Carson II, Barry L. Nelson, David M. Nicol: 
 * Discrete-Transaction System Simulation, Fourth Edition, Prentice Hall, 2005.
 * 
 * However, the code is not exactly the same ;-)
 *  
 ******************************************************************************/
package main.java.workload;

import java.io.File;
import java.io.IOException;
import java.util.LinkedList;
import java.util.Map;
import java.util.Set;
import java.util.SortedMap;
import java.util.SortedSet;
import java.util.TreeMap;
import java.util.TreeSet;
import java.util.Map.Entry;
import java.util.concurrent.TimeUnit;

import org.apache.commons.math3.distribution.EnumeratedIntegerDistribution;




//import org.apache.commons.math3.distribution.ExponentialDistribution;
import umontreal.iro.lecuyer.randvar.ExponentialGen;
import umontreal.iro.lecuyer.randvar.RandomVariateGen;
import umontreal.iro.lecuyer.rng.MRG32k3a;
import umontreal.iro.lecuyer.simevents.Accumulate;
import umontreal.iro.lecuyer.simevents.Event;
import umontreal.iro.lecuyer.simevents.Sim;
import umontreal.iro.lecuyer.stat.Tally;

import com.jramoyo.io.IndexedFileReader;

import main.java.cluster.Cluster;
import main.java.db.Database;
import main.java.db.Tuple;
import main.java.entry.Global;
import main.java.metric.Metric;
import main.java.repartition.ClusterIdMapper;
import main.java.repartition.DataMovement;
import main.java.repartition.WorkloadFileGenerator;

public class WorkloadExecutor {

	static SortedMap<Integer, Map<Integer, Integer>> frequentTrMap;
	static EnumeratedIntegerDistribution trDistribution;
	static SortedSet<Integer> deletedTuples;
    static long targeted_transactions;
    static long transactions;
    static int trTypePointer;
    static int[] trType;
    static int simTimeInHours;
    static double clock; 	   
	
	static RandomVariateGen genArr;
	static RandomVariateGen genServ;

	static LinkedList<Transaction> waitList = new LinkedList<Transaction> ();
	static LinkedList<Transaction> servList = new LinkedList<Transaction> ();

	static Tally transactionWaits = new Tally("Waiting times");
	static Accumulate totalWait = new Accumulate("Size of queue");

	// Statistic collectors
	static int batch_total_transactions;
	static double batch_total_response_time;
	
    public WorkloadExecutor() {    	
        transactions = 0;
        trTypePointer = 0;
        simTimeInHours = 0;
        clock = 0.0;
               
        frequentTrMap = new TreeMap<Integer, Map<Integer, Integer>>();
        trDistribution = new EnumeratedIntegerDistribution(Global.tpccTrTypes, Global.tpccTrProbabilities);
        deletedTuples = new TreeSet<Integer>();
        
        MRG32k3a rand = new MRG32k3a();
        
        long[] seed = new long[10];
        for(int i = 0; i < 10; i++)
        	seed[i] = (long) Global.repeated_runs+1;

        rand.setSeed(seed);
        trDistribution.reseedRandomGenerator(seed[0]);
        
        genArr = new ExponentialGen(rand, Global.meanInterArrivalTime); // lambda        
		genServ = new ExponentialGen(rand, Global.meanServiceTime); // mu
		
        targeted_transactions = (int)(Global.transactions - Global.bufferSize)/Global.workloadBatches;
        trType = trDistribution.sample((int) Global.transactions);
        
        batch_total_transactions = 0;
        batch_total_response_time = 0.0;
    }
	
	@SuppressWarnings("unused")
	private static double exp(double mean) {
        return -mean * Math.log(Global.rand.nextDouble());
    }

	// Stream a single Transaction from the specified stored file
	public static Transaction streamOneTransaction(Database db, Cluster cluster, WorkloadBatch wb) {
		Transaction tr = null;		
		
		++trTypePointer;		
		int type = trType[trTypePointer - 1];
		
		++Global.tpccLineNumbers[type -1];
		int line = Global.tpccLineNumbers[type - 1];
		
		if(!wb.getTrMap().containsKey(type) || !frequentTrMap.containsKey(type)) {
			wb.getTrMap().put(type, new TreeMap<Integer, Transaction>());
			frequentTrMap.put(type, new TreeMap<Integer, Integer>());
		}		
		
		if(frequentTrMap.get(type).containsKey(line)) {
			
			int tr_id = frequentTrMap.get(type).get(line);
			tr = wb.getTrMap().get(type).get(tr_id);
			
			tr.incTr_frequency();
			tr.incTr_temporalWeight();
			tr.calculateDTImapct();
		} else {
			
	    	File trFile = new File(Global.wrl_dir+"/"+"run"+Global.repeated_runs+"/"+"t"+type+".tr");
			IndexedFileReader reader = null;
			SortedMap<Integer, String> trStream = null;		
					
			try {	
				reader = new IndexedFileReader(trFile);
				trStream = reader.readLines(line, line);
			} catch (IOException e) {
				e.printStackTrace();
			} finally {
				try {
					reader.close();
				} catch (IOException e) {
					e.printStackTrace();
				}
			}
					
			// Process a single stream to convert it to a Transaction
			String[] dataStream = null;
			Set<Integer> trDataSet = new TreeSet<Integer>();
			
			for(Entry<Integer, String> entry : trStream.entrySet()) {
				dataStream = entry.getValue().split(" ");
				
				for(String d : dataStream) {					
					int tpl_id = Integer.parseInt(d);				
					
					if(!deletedTuples.contains(tpl_id)) {
					
						String[] parts = cluster.breakDataId(tpl_id);					
						int tpl_pk = Integer.parseInt(parts[0]);
						int tbl_id = Integer.parseInt(parts[1]);
						
						Tuple tpl = db.getTupleById(tbl_id, tpl_id);					
						int data_id = 0;						
						
						if(tpl.getTuple_action().equals("insert")) {
							data_id = cluster.insertData(tpl_id);
							trDataSet.add(data_id); // Insert into Cluster, already in the Database
							
						} else if (tpl.getTuple_action().equals("delete")) {
							cluster.deleteData(tpl_id); // Remove from Cluster
							deletedTuples.add(tpl_id);
							db.deleteTupleByPk(tbl_id, tpl_pk); // Remove from Database
							
						} else {
							data_id = cluster.getDataIdFromTupleId(tpl_id);
							trDataSet.add(data_id);
						}
					}
				}
			}
			
			// Create a new Transaction
			++transactions;
			++Global.global_trSeq;
			tr = new Transaction(Global.global_trSeq, type, trDataSet);							
				
			// Add the newly created Transaction in the Workload Transaction map	
			wb.getTrMap().get(type).put(Global.global_trSeq, tr);
			
			// Add an entry to check for duplicate Transaction in future
			frequentTrMap.get(type).put(line, Global.global_trSeq);
						
			//tr.calculateSpans(cluster);
			//tr.calculateDTImapct();
		}
		
		// Statistic collection
		batch_total_transactions += 1;			
		
		return tr;
	}
		
	public static void processTransaction(Transaction tr) {
		// Sleep for x milliseconds
    	try {    		
    		TimeUnit.MILLISECONDS.sleep(tr.getTr_serverSpanCost());
    		
    		double response = (double)tr.getTr_serverSpanCost() + tr.getTr_waiting_time();
    		double avg_response = (response + tr.getTr_response_time())/2;
    		double round_avg_response = Math.round(avg_response * 100.0)/100.0;
    		
    		batch_total_response_time += round_avg_response;
    		
    		tr.setTr_response_time(round_avg_response);
    		tr.setProcessed(true);
    		
    	} catch(InterruptedException ex) {
    	    Thread.currentThread().interrupt();
    	}
	}

	private void simulateOneRun(Database db, Cluster cluster, WorkloadBatch wb, double timeHorizon) {
		Sim.init();
		
		new EndOfSim().schedule(timeHorizon);
		new Arrival(db, cluster, wb).schedule(genArr.nextDouble());
		
		Sim.start();
	}
	
	public void execute(Database db, Cluster cluster) {
		
		Global.LOGGER.info("Generating a minimum of "+targeted_transactions+" transactions per workload batch.");
						
		int batch = 1;		
		Global.global_trSeq = 0;
		Slide slide = new Slide();
		
		while(batch <= Global.workloadBatches) {
			Global.LOGGER.info("=============================================================================");
			Global.LOGGER.info("Streaming workload batch "+(batch)+" ...");
			
			WorkloadBatch wb = new WorkloadBatch(batch);			
			transactions = 0;
			
			while(transactions <= targeted_transactions) {
				int slideStartTr = Global.global_trSeq;

				int slotTransactions = 0;
				while(slotTransactions <= (slide.end - slide.start)) {
					int slotStartTr = Global.global_trSeq;
					this.simulateOneRun(db, cluster, wb, (Global.simulationPeriod/24)/1); // Set to 1h slot									
					
					slotTransactions += (Global.global_trSeq - slotStartTr);					
					Global.LOGGER.info("Slot contains "+(Global.global_trSeq - slotStartTr)+" transactions.");
					
					//Global.LOGGER.info(transactionWaits.report());
					//Global.LOGGER.info(totalWait.report());
				} // end-while() -- 1h slot				
						 
				Global.LOGGER.info(".............................................................................");
				Global.LOGGER.info("Slide contains "+(Global.global_trSeq - slideStartTr)+" transactions.");				
				Global.LOGGER.info(".............................................................................");
				
				slide.start += Global.slidingWindow/2;
				slide.end += Global.slidingWindow/2;

				// This will generate frequent/duplicate transactions
				trTypePointer /= 2;
				
				for(int i = 0; i < Global.tpccLineNumbers.length; i++) {
					Global.tpccLineNumbers[i] /= 2; 
				}
			} // end-while -- 500 Transactions slot
			
			Global.LOGGER.info("Total "+transactions+" unique transactions have streamed in current workload batch.");
			Global.LOGGER.info("Total "+batch_total_transactions+" transactions have processed by the Transaction Coordinator.");
			Global.LOGGER.info("Total time: "+(Sim.time() / 3600)+" Hours");
			
			Global.LOGGER.info("-----------------------------------------------------------------------------");
						
			// Statistic preparation, collection, and reporting
			cluster.updateLoad();
			
			wb.setWrl_totalTransaction((int) transactions);
			wb.calculateDTI(cluster);			
			wb.calculateThroughput(batch_total_transactions);
			wb.calculateResponseTime(batch_total_transactions, batch_total_response_time);
			
			Metric.collect(cluster, wb);			
			Metric.report();
			
			// Perform workload-aware data partitioning
			if(Global.workloadAware) {
			
				// Initialisation
				WorkloadFileGenerator workloadFileGenerator = new WorkloadFileGenerator();
				ClusterIdMapper cluster_id_mapper = new ClusterIdMapper();
				DataMovement data_movement = new DataMovement();
				
				
				
			}
			
			// Moving forward
			simTimeInHours += 1;			
			batch_total_transactions = 0;

			++batch;
		} // end-while() -- 1000 Transactions batch
				
		Global.LOGGER.info("Total "+Global.global_trSeq+" unique transactions have generated.");
		Global.LOGGER.info("Total time: "+simTimeInHours+" Hours");
	}
	
} // end-Class

//=======================================================================================
class Arrival extends Event {
	Database db;
	Cluster cluster;
	WorkloadBatch wb;
	
	public Arrival(Database db, Cluster cluster, WorkloadBatch wb) {
		this.db = db;
		this.cluster = cluster;
		this.wb = wb;
	}

	public void actions() {
		new Arrival(this.db, this.cluster, this.wb).schedule(WorkloadExecutor.genArr.nextDouble()); // Next arrival
		//new Arrival(this.cluster).schedule(WorkloadExecutor.genArr.sample()); // Next arrival
		
		// A Transaction has just arrived
		Transaction tr = WorkloadExecutor.streamOneTransaction(this.db, this.cluster, this.wb);
		
		tr.setTr_arrival_time(Sim.time());			
		tr.setTr_service_time(WorkloadExecutor.genServ.nextDouble());
		//tr.setTr_service_time(WorkloadExecutor.genServ.sample());
		
		if (WorkloadExecutor.servList.size() > 0) { // Must join the queue.
			WorkloadExecutor.waitList.addLast(tr);
			WorkloadExecutor.totalWait.update(WorkloadExecutor.waitList.size());
		} else { // Starts service
			tr.setTr_waiting_time(0.0);
			
			WorkloadExecutor.transactionWaits.add(0.0);			
			WorkloadExecutor.servList.addLast(tr);
			
			new Departure().schedule(tr.getTr_service_time());
		}
	}
}

//=======================================================================================
class Departure extends Event {
	public void actions() {
		Transaction transaction = WorkloadExecutor.servList.removeFirst();
		WorkloadExecutor.processTransaction(transaction);
		
		if (WorkloadExecutor.waitList.size() > 0) {
			// Starts service for next one in queue.
			Transaction tr = WorkloadExecutor.waitList.removeFirst();
			tr.setTr_waiting_time(Sim.time() - tr.getTr_arrival_time());
			
			WorkloadExecutor.totalWait.update(WorkloadExecutor.waitList.size());
			WorkloadExecutor.transactionWaits.add(Sim.time() - tr.getTr_arrival_time());			
			
			WorkloadExecutor.processTransaction(tr);			
			WorkloadExecutor.servList.addLast(tr);
			
			new Departure().schedule(tr.getTr_service_time());
		}
	}
}

//=======================================================================================
class EndOfSim extends Event {
	public void actions() {
		Sim.stop();
	}
}

//=======================================================================================
class Slide {
	public long start; 
	public long end;
	
	public Slide() {start = 0; end = Global.slidingWindow;}
}